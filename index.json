{"project": "scikit-learn", "project_url": "scikit-learn.org/", "show_commit_url": "https://github.com/scikit-learn/scikit-learn/commit/", "hash_length": 8, "revision_to_hash": {"382": "e6989efd71a2adddd03979d1fe7a2e82e37ea51f", "395": "8ff9fc895bd6032636e3716f02773fdcd9cdd3d3", "587": "0e1faafec9871df73e875a0aadfcb67ec578c0e5", "590": "a40d325cec40da6cbcff8193a4ab4890823dfc76", "646": "ddc6d8f80dcf0a6cdd606efdefb89211a4dc7e9d", "745": "8a4bc2f03733e530591d6641f266a60670a373f1", "755": "8216797c4b1abca9dafd8de9d65472d32450b389", "811": "c7208c1a43335179ccddafc7748c1d7224e904fc", "813": "47890ac823314f1a9e2920dff7575850af56c273", "826": "b573fc0dcbfc2528807b5f0f8c0bc719c25d36f4", "998": "65d06f830ec6604b44d1a0510255868a8f762e3a", "1016": "9072aa593d76262fe445cf492ffac77e853501ea", "1131": "959e267898090e3c68ee118d5048afad124ff61d", "1207": "c83447b72c4f48ceb8249ea394ebf042618b8a2a", "1288": "f13dba15e3d56455c58867685ec554755a346c32", "1915": "d6b4444bbcc54a241cc955a5ceea80be15e7db2b", "1917": "60589710bd64e1fb2ede4d34d7fbb57e83892c86", "1919": "eba9984f735478d47c956ede42bdefd28aa6f9f6", "1969": "0f148e0011fb873bcd70cb3cc01690e7d621f670", "1977": "dc72677a9c13a656cda8be4b23cd897b56109b4b", "2701": "2c3d9e2fce5d2bae27e10657aa3c7ff45c39b190", "2711": "87741a7c65768464eb15f0976ed4bf6312795e7f", "2743": "03a85c19ac2854f2a33f613f87e81fd5f4560f55", "3102": "c07f9574c902b68744434d7b43f7394e0801d64e", "3151": "8a195624128da773c7d584d9352f65d8241cc92d", "3212": "897201083fd584a310cb8a2870704470dc28474a", "3289": "bdf3332f9694f8ecbdcf7ab0391989e24ac13f88", "3741": "5a1e1f48433ba867fb035b9dc31882f8d90f7744", "3905": "af6ab92b3bc0286e401218631859ee50f8be23f7", "4037": "f7c9f24511d9b32add23e75bbf0a2a6c223d932f", "4054": "8b2aaf069306d6b61b49a29d32123e69991c153b", "4684": "cf5c72eb9dc7696b5fac61466605b2860942946e", "4696": "3b48abd5fb0fa4f87c09f6b21d0d1f0e8b7873e4", "6177": "3e3872cde115550b75bb25c47c109b8bfd070eab", "6225": "3f1ea662ee1b1b08cee63cc31e4e3e36ec532208", "6511": "bfd36aa504078ce58f727f7f37e17349ab290e7d", "7872": "4533aa33daa35dd68c6d433b1d3560ff2b65b252", "7904": "34334f5ce6b1f166efda8652310133f9fc36ed04", "7919": "79749fd2939781e201191ef081143d8a575984e7", "9331": "34c2904a95a707c6e6148480a7e2c86a0f7ad86b", "9349": "73fdf6a9c982758be6da71a932ec4a3613eccbbf", "9357": "4ae44b0fe10b3ddf8390cfa8deae4dec45c40666", "9369": "7eb39fa0dc43ce485d3af2857c587811332eb148", "9799": "114822b1e18c9d7f887c58b8a3b2c279bdce6d35", "10413": "4bc8822c846de0d3b70d006ea32235d4375a575b", "10436": "0fede44fb39d691e873d58a4210452aa93c462a5", "10457": "b9ed384195df7b8d7824eac42f7b1bee58ef321c", "10778": "0dd2e39c1f7aec6830e4348fa63a04939252a0a1", "12368": "3e89aa5f42519d7f0230b99948553a8eb33dc1f4", "12373": "86e8b0d2a3533253a7082591f572d73897c02a2c", "12748": "8075887585b0449b6e87ee54c2ca4dbd56960e1e", "14515": "fc0b766ceca487504b040896124a3d809af2975b", "14698": "d13928cc0653f52de55e22118915b0c5bcba13d7", "14725": "34c4908369968dd0f77897ec9dd8c227e7545478", "16940": "bc8666f60f2c8c9ba16b30fbe0b342c3b94213e6", "17074": "68280fb4254b0781a66a1d2689708068799f0bbe", "17075": "b4e8b3ca4366901998c116540902d2687e0a5450", "17279": "518002955b0d6539f8f5e2710b9cefb178cc8ee2", "17645": "d4906939b1ef86657e6617d8fa078a0fbe0c2472", "17934": "2068ff2fd94abe4f14b0334eb4372a64b268f6b4", "19198": "4cc0235ec1ee654ea85cf465d280d33bcb1db20c", "19199": "09dc09a1e9d9088c2cb783c818980f5509d77a11", "19375": "df9f90cfa8795b6d85056f70177fb783d6ecafda", "19504": "bb39b493ef084a4f362d77163c2ca506790c38b6", "19920": "25082e522c90fa9184789f6bc450278b3e18fdda", "20502": "c0c2c737971b52e04b1f6516dfa1bfb05b30f4fd", "20509": "cd12906cabf3576a8c236a4128e959360037dde0", "20952": "918005fd5441650ae4a49b510bcabff69ae898bf", "20955": "b5383488c4b8b97b000585e61ed4e2178fa84d36", "21113": "da4f480a6adf5fed30a42500fe0e5a21c404ac2a", "21126": "82fb053536803f172def9f64e0d62151529173a0", "21322": "2999a2f544cd56575d940d7ab359819b392cccae", "21323": "3c546fd1226a895f68d317d2430daa71fc13e093", "21601": "ea042f1485d5fe45bcf2475c3070cab4e5ac3381", "21602": "51a765acfa4c5d1ec05fc4b406968ad233c75162", "22265": "4d9fab55b9e14e01a7d13344a2612ed802d0c113", "22268": "b687ab371d990373c4a599399172cf31d2f0c350", "22283": "cef2b62701f80ff50a37528b5337dd9a96f0069e", "22393": "38030a00a7f72a3528bd17f2345f34d1344d6d45", "22702": "a5ab948cbc366d705b1f8db8687c7162f51de22d", "23197": "759f4637f9f9471cf4218b9dffc00b464790485b", "23280": "36bc053a69ac5b9ba5a54cb2bd19adb33dcde50e", "23283": "62523372fc6331fc55df73a94d65bfa48c45c193", "23307": "83816c2a95e2ae3c4b3546912de4f4266e0c230f", "23480": "81ba62fe053d56e228ce097cbca91bc5de2e3f82", "23768": "b661a9c81930429cba4a56af291ce2bf8c59f8c9", "24417": "8c439fbe8c340389d7f9d99884180b2e7b21a79f", "24644": "eb6764936c9558553f7a7203a6aaa0ddc6497875", "24809": "f659f5539f9d36ebec4e1d98538919b55299bba4", "25159": "55bf5d93e5674f13a1134d93a11fd0cd11aabcd1", "25261": "7389dbac82d362f296dc2746f10e43ffa1615660", "25563": "7b136e92acf49d46251479b75c88cba632de1937", "25750": "ee986788cbd3256f0c36d2ddae155d8ca8f7be1c", "25758": "be2f62b2bfb40747a2dab20f29a341879b247a3c", "25766": "60eb00c72541b42697fa017fdfc74935299fc455", "25767": "93b19b04d3c81f9824b23e1b910126d51f3cd342", "25841": "a243d96336cb4f50ca3635b3062a273f3dc5183a", "25874": "b7b4d3e2f1a65bcb6d40431d3b61ed1d563c9dab", "25910": "e8602bc04e5c7ab32e6acb887b68172098f7f1e2", "26278": "1495f69242646d239d89a5713982946b8ffcf9d9", "26284": "bddd9257f39f190fec3d72872cff73c2b3cc2734", "26819": "db6c12fc117500a751799a3082d1503b65183920", "26820": "e36317b50d70453622ac6d0324a700816bad21c1", "26881": "d39134bc77d9f9a5a0316e21ee32ac3f9683da3d", "26890": "f1f765f476c3cb3e0a882324f8ed67763d76ed26", "26944": "0a56df6dbbe4f1a56cb11d132e43641d7358dd7e", "26953": "5f3c3f0378f2f30f3c4340bd9bf1e211e96d5c3c", "27063": "e5698bde9a8b719514bf39e6e5d58f90cfe5bc01", "27300": "4b7331eeb746b3facb4d70e1760c58ebe8b47f2e", "27325": "daefc22f832177dcbb690369058e0ca776944188", "27501": "a1261a7e18c19bb3dfc8d739a6512c6f671d9e79", "27574": "22a7d5bc722b0430908f202e3ea40aa2ba1a0361", "27607": "483cd3eaa3c636a57ebb0dc4765531183b274df0", "27677": "fd237278e895b42abe8d8d09105cbb82dc2cbba7", "28089": "0fb307bf39bbdacd6ed713c00724f8f871d60370", "28258": "d2cd2540418d3ff66b324ec18566dbe0b5991b40", "28264": "ab3dc9fdf31f35854b390168ac68fb304951305f"}, "revision_to_date": {"382": 1264603663000, "395": 1265024295000, "587": 1269007740000, "590": 1269244318000, "646": 1270698297000, "745": 1272878771000, "755": 1272909852000, "811": 1273294444000, "813": 1273337313000, "826": 1273550555000, "998": 1277561666000, "1016": 1277824675000, "1131": 1279558872000, "1207": 1280235748000, "1288": 1281104090000, "1915": 1286358759000, "1917": 1286371631000, "1919": 1286373337000, "1969": 1286782666000, "1977": 1286812002000, "2701": 1292596297000, "2711": 1292968871000, "2743": 1294669325000, "3102": 1298808434000, "3151": 1299078131000, "3212": 1299680572000, "3289": 1300670714000, "3741": 1302729559000, "3905": 1304345121000, "4037": 1305108339000, "4054": 1305126138000, "4684": 1309505367000, "4696": 1309545528000, "6177": 1316527045000, "6225": 1316642399000, "6511": 1318977607000, "7872": 1326216418000, "7904": 1326287141000, "7919": 1326426441000, "9331": 1336328405000, "9349": 1336407543000, "9357": 1336426836000, "9369": 1336524866000, "9799": 1341415689000, "10413": 1346780365000, "10436": 1346788092000, "10457": 1346981442000, "10778": 1349734773000, "12368": 1358801181000, "12373": 1358806382000, "12748": 1361636276000, "14515": 1375060310000, "14698": 1375915741000, "14725": 1375972053000, "16940": 1401972588000, "17074": 1404172403000, "17075": 1404240383000, "17279": 1405354512000, "17645": 1406899808000, "17934": 1409835738000, "19198": 1425667359000, "19199": 1425681522000, "19375": 1427396256000, "19504": 1429027572000, "19920": 1436588422000, "20502": 1445007457000, "20509": 1445013082000, "20952": 1445953225000, "20955": 1445958667000, "21113": 1446753465000, "21126": 1446818031000, "21322": 1450749753000, "21323": 1450761225000, "21601": 1455802186000, "21602": 1455802246000, "22265": 1473791605000, "22268": 1473799038000, "22283": 1473882154000, "22393": 1475007587000, "22702": 1478902517000, "23197": 1497903977000, "23280": 1499947923000, "23283": 1499952302000, "23307": 1500281648000, "23480": 1502470027000, "23768": 1505921382000, "24417": 1531668719000, "24644": 1535551327000, "24809": 1537887812000, "25159": 1542875805000, "25261": 1545209521000, "25563": 1551431215000, "25750": 1556378170000, "25758": 1556603560000, "25766": 1556634531000, "25767": 1556635319000, "25841": 1557442357000, "25874": 1557893876000, "25910": 1558616196000, "26278": 1564406958000, "26284": 1558572641000, "26819": 1573637668000, "26820": 1573476950000, "26881": 1574093837000, "26890": 1574244189000, "26944": 1574933073000, "26953": 1575301264000, "27063": 1577976510000, "27300": 1582908441000, "27325": 1583315487000, "27501": 1588010313000, "27574": 1588689638000, "27607": 1589270282000, "27677": 1589872045000, "28089": 1592994930000, "28258": 1599747257000, "28264": 1600274505000}, "params": {"arch": ["x86_64"], "cpu": ["Intel Core Processor (Haswell, no TSX)"], "machine": ["sklearn-benchmark"], "num_cpu": ["8"], "os": ["Linux 4.15.0-20-generic"], "ram": ["16424684"], "python": ["3.8"], "numpy": [""], "scipy": [""], "cython": [""], "joblib": [""], "threadpoolctl": [""], "branch": ["master"]}, "graph_param_list": [{"arch": "x86_64", "cpu": "Intel Core Processor (Haswell, no TSX)", "machine": "sklearn-benchmark", "num_cpu": "8", "os": "Linux 4.15.0-20-generic", "ram": "16424684", "python": "3.8", "numpy": "", "scipy": "", "cython": "", "joblib": "", "threadpoolctl": "", "branch": "master"}], "benchmarks": {"cluster.KMeansBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "cluster.KMeansBenchmark.peakmem_fit", "param_names": ["representation", "algorithm", "init"], "params": [["'dense'", "'sparse'"], ["'full'", "'elkan'"], ["'random'", "'k-means++'"]], "setup_cache_key": "cluster:16", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "44996b30a90296496e9fbd453654a32aa6c2909019acefc1ecae8d8eb4609d15"}, "cluster.KMeansBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "cluster.KMeansBenchmark.peakmem_predict", "param_names": ["representation", "algorithm", "init"], "params": [["'dense'", "'sparse'"], ["'full'", "'elkan'"], ["'random'", "'k-means++'"]], "setup_cache_key": "cluster:16", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "510a8c6e9aad0a40d0f8a0b5e80a5e07f0d8151c5efce1d37b2fc63c39bb32e5"}, "cluster.KMeansBenchmark.peakmem_transform": {"code": "class Transformer:\n    def peakmem_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "cluster.KMeansBenchmark.peakmem_transform", "param_names": ["representation", "algorithm", "init"], "params": [["'dense'", "'sparse'"], ["'full'", "'elkan'"], ["'random'", "'k-means++'"]], "setup_cache_key": "cluster:16", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "70a2cb80848550589a4507ca5a90210a9df9621d3fbe9a665f3fe215d263ac25"}, "cluster.KMeansBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "cluster.KMeansBenchmark.time_fit", "number": 0, "param_names": ["representation", "algorithm", "init"], "params": [["'dense'", "'sparse'"], ["'full'", "'elkan'"], ["'random'", "'k-means++'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "cluster:16", "timeout": 500, "type": "time", "unit": "seconds", "version": "a7f0a87fd088d4fcaf9d60644c6c653c75d1261c10be350e2ad325cbbade2aaf", "warmup_time": 1}, "cluster.KMeansBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "cluster.KMeansBenchmark.time_predict", "number": 0, "param_names": ["representation", "algorithm", "init"], "params": [["'dense'", "'sparse'"], ["'full'", "'elkan'"], ["'random'", "'k-means++'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "cluster:16", "timeout": 500, "type": "time", "unit": "seconds", "version": "75e2bf5cab4c55598df2a5964235bb56158f1fd58fbe88037bbeb27518941430", "warmup_time": 1}, "cluster.KMeansBenchmark.time_transform": {"code": "class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "cluster.KMeansBenchmark.time_transform", "number": 0, "param_names": ["representation", "algorithm", "init"], "params": [["'dense'", "'sparse'"], ["'full'", "'elkan'"], ["'random'", "'k-means++'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "cluster:16", "timeout": 500, "type": "time", "unit": "seconds", "version": "e89a4fa90042114d4904732525795519e353e717562e734ef51e1a17fdb37ee2", "warmup_time": 1}, "cluster.KMeansBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "cluster.KMeansBenchmark.track_test_score", "param_names": ["representation", "algorithm", "init"], "params": [["'dense'", "'sparse'"], ["'full'", "'elkan'"], ["'random'", "'k-means++'"]], "setup_cache_key": "cluster:16", "timeout": 500, "type": "track", "unit": "unit", "version": "3c3af2849d342cebc878a7ac97ad167c9db632ac6e72ee902c1f0def6ef488b8"}, "cluster.KMeansBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "cluster.KMeansBenchmark.track_train_score", "param_names": ["representation", "algorithm", "init"], "params": [["'dense'", "'sparse'"], ["'full'", "'elkan'"], ["'random'", "'k-means++'"]], "setup_cache_key": "cluster:16", "timeout": 500, "type": "track", "unit": "unit", "version": "0b1413310939b67bdbb6490a3b9ccc19d0f4bf68615214df8463596abc27cd29"}, "cluster.MiniBatchKMeansBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "cluster.MiniBatchKMeansBenchmark.peakmem_fit", "param_names": ["representation", "init"], "params": [["'dense'", "'sparse'"], ["'random'", "'k-means++'"]], "setup_cache_key": "cluster:63", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "27a823a52563653b94cee28aee3234581dfdc555a0360637498b08166bc1d9ad"}, "cluster.MiniBatchKMeansBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "cluster.MiniBatchKMeansBenchmark.peakmem_predict", "param_names": ["representation", "init"], "params": [["'dense'", "'sparse'"], ["'random'", "'k-means++'"]], "setup_cache_key": "cluster:63", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "41369a0e02bed31162a08bdf8f5fa2bac1ece3644dd88e977837dc5ebfc9449c"}, "cluster.MiniBatchKMeansBenchmark.peakmem_transform": {"code": "class Transformer:\n    def peakmem_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "cluster.MiniBatchKMeansBenchmark.peakmem_transform", "param_names": ["representation", "init"], "params": [["'dense'", "'sparse'"], ["'random'", "'k-means++'"]], "setup_cache_key": "cluster:63", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "3809620b44698eaa55ea358f5867374ef6b4f0dc3b7b3fb2bd28de40db441d22"}, "cluster.MiniBatchKMeansBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "cluster.MiniBatchKMeansBenchmark.time_fit", "number": 0, "param_names": ["representation", "init"], "params": [["'dense'", "'sparse'"], ["'random'", "'k-means++'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "cluster:63", "timeout": 500, "type": "time", "unit": "seconds", "version": "f3fda2b1a621d39ede4bc2103dede0fde92fbd210d39d015ea61934bfccfc7c2", "warmup_time": 1}, "cluster.MiniBatchKMeansBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "cluster.MiniBatchKMeansBenchmark.time_predict", "number": 0, "param_names": ["representation", "init"], "params": [["'dense'", "'sparse'"], ["'random'", "'k-means++'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "cluster:63", "timeout": 500, "type": "time", "unit": "seconds", "version": "23ec70517acd7b8eba2cdc3cc9c57604c51b5639b32fffc729a4f20a24b06d72", "warmup_time": 1}, "cluster.MiniBatchKMeansBenchmark.time_transform": {"code": "class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "cluster.MiniBatchKMeansBenchmark.time_transform", "number": 0, "param_names": ["representation", "init"], "params": [["'dense'", "'sparse'"], ["'random'", "'k-means++'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "cluster:63", "timeout": 500, "type": "time", "unit": "seconds", "version": "80d12f75cd37166bced2570d27f5250389ec6f7df8982e734288778fa0067280", "warmup_time": 1}, "cluster.MiniBatchKMeansBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "cluster.MiniBatchKMeansBenchmark.track_test_score", "param_names": ["representation", "init"], "params": [["'dense'", "'sparse'"], ["'random'", "'k-means++'"]], "setup_cache_key": "cluster:63", "timeout": 500, "type": "track", "unit": "unit", "version": "b15e6712b29288eb2cf5fc5bd336054544a9002dfab16a6e299d6f992154edc9"}, "cluster.MiniBatchKMeansBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "cluster.MiniBatchKMeansBenchmark.track_train_score", "param_names": ["representation", "init"], "params": [["'dense'", "'sparse'"], ["'random'", "'k-means++'"]], "setup_cache_key": "cluster:63", "timeout": 500, "type": "track", "unit": "unit", "version": "858db1057ab8f972235963a5c0e46e44c848881c70c927afddb64243168000b0"}, "decomposition.DictionaryLearningBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.DictionaryLearningBenchmark.peakmem_fit", "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "setup_cache_key": "decomposition:44", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "190e4b8153b3607376247c0a64f4c1d8aa385789d8f2eb6e65be2006fc0f21a0"}, "decomposition.DictionaryLearningBenchmark.peakmem_transform": {"code": "class Transformer:\n    def peakmem_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.DictionaryLearningBenchmark.peakmem_transform", "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "setup_cache_key": "decomposition:44", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "b5d9d222d5acbb1ba622ed388de04f7bc205f57e0ec5112c0792edad34ad712a"}, "decomposition.DictionaryLearningBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "decomposition.DictionaryLearningBenchmark.time_fit", "number": 0, "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "decomposition:44", "timeout": 500, "type": "time", "unit": "seconds", "version": "3b1cb33cf7cae38b9084e69ee8840abd4a0209018cf63db5ae6d22e2915ecc04", "warmup_time": 1}, "decomposition.DictionaryLearningBenchmark.time_transform": {"code": "class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "decomposition.DictionaryLearningBenchmark.time_transform", "number": 0, "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "decomposition:44", "timeout": 500, "type": "time", "unit": "seconds", "version": "78044cbc0c8ef76c735aecaaee722df32400100fd621a5cb73d6207aad3736e8", "warmup_time": 1}, "decomposition.DictionaryLearningBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.DictionaryLearningBenchmark.track_test_score", "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "setup_cache_key": "decomposition:44", "timeout": 500, "type": "track", "unit": "unit", "version": "dffe185efa679099c300a200d5f1548d7025bb14150cc130ee9594dbca6a08f4"}, "decomposition.DictionaryLearningBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.DictionaryLearningBenchmark.track_train_score", "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "setup_cache_key": "decomposition:44", "timeout": 500, "type": "track", "unit": "unit", "version": "e132e82fb0e747524994e89713ee6cc7c16e11ac696381769ba0b7a865f809ee"}, "decomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit", "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "setup_cache_key": "decomposition:75", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "7413c15d31d1af9d57b047e1eb9793e8fd9442dd4852f9bbf7944520b6cde539"}, "decomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform": {"code": "class Transformer:\n    def peakmem_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform", "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "setup_cache_key": "decomposition:75", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "9583e478652b4aa7adf26052264d261ed51218bfb02c392a8dd5afa6f0be9067"}, "decomposition.MiniBatchDictionaryLearningBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "decomposition.MiniBatchDictionaryLearningBenchmark.time_fit", "number": 0, "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "decomposition:75", "timeout": 500, "type": "time", "unit": "seconds", "version": "fbedaca37b514fdecdc98bf0d5e99cc18f4e81adafa0501bf2cb62b395cff80b", "warmup_time": 1}, "decomposition.MiniBatchDictionaryLearningBenchmark.time_transform": {"code": "class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "decomposition.MiniBatchDictionaryLearningBenchmark.time_transform", "number": 0, "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "decomposition:75", "timeout": 500, "type": "time", "unit": "seconds", "version": "04e8b945e21dc8f7471c0bb5d123098da806565bcd07f0ae5c78c42492669af1", "warmup_time": 1}, "decomposition.MiniBatchDictionaryLearningBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.MiniBatchDictionaryLearningBenchmark.track_test_score", "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "setup_cache_key": "decomposition:75", "timeout": 500, "type": "track", "unit": "unit", "version": "3254255804120abf8154e7fb27ecc492ebf763052bc0b660de4ae3579648a704"}, "decomposition.MiniBatchDictionaryLearningBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.MiniBatchDictionaryLearningBenchmark.track_train_score", "param_names": ["fit_algorithm", "n_jobs"], "params": [["'lars'", "'cd'"], ["1", "4"]], "setup_cache_key": "decomposition:75", "timeout": 500, "type": "track", "unit": "unit", "version": "68f98a3244c8ad2dec8817d68a45e5c8185c6b2487bca85e5302d103e6bdf725"}, "decomposition.PCABenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.PCABenchmark.peakmem_fit", "param_names": ["svd_solver"], "params": [["'full'", "'arpack'", "'randomized'"]], "setup_cache_key": "decomposition:17", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "42a67cdbc3312242272375647a4c8921d248f3932a1f45439cd1ee6fb5bfa1ac"}, "decomposition.PCABenchmark.peakmem_transform": {"code": "class Transformer:\n    def peakmem_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.PCABenchmark.peakmem_transform", "param_names": ["svd_solver"], "params": [["'full'", "'arpack'", "'randomized'"]], "setup_cache_key": "decomposition:17", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "7d07701a485134ed88af807935021a90621db5698a9246bc6da7a2088a7f79c2"}, "decomposition.PCABenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "decomposition.PCABenchmark.time_fit", "number": 0, "param_names": ["svd_solver"], "params": [["'full'", "'arpack'", "'randomized'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "decomposition:17", "timeout": 500, "type": "time", "unit": "seconds", "version": "0cbc79d745e8e837e60d0fb6ce4c28d169a4f3d0374aa4c41c1af06f20fd00ae", "warmup_time": 1}, "decomposition.PCABenchmark.time_transform": {"code": "class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "decomposition.PCABenchmark.time_transform", "number": 0, "param_names": ["svd_solver"], "params": [["'full'", "'arpack'", "'randomized'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "decomposition:17", "timeout": 500, "type": "time", "unit": "seconds", "version": "32433581033fb7c58370c65de59d3bc16af930d20e4b5b1de444aa4cf264b1f1", "warmup_time": 1}, "decomposition.PCABenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.PCABenchmark.track_test_score", "param_names": ["svd_solver"], "params": [["'full'", "'arpack'", "'randomized'"]], "setup_cache_key": "decomposition:17", "timeout": 500, "type": "track", "unit": "unit", "version": "76c2fa737efe3334a87e39743a537297128e25318011c3e3ffec6d548f265b4c"}, "decomposition.PCABenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "decomposition.PCABenchmark.track_train_score", "param_names": ["svd_solver"], "params": [["'full'", "'arpack'", "'randomized'"]], "setup_cache_key": "decomposition:17", "timeout": 500, "type": "track", "unit": "unit", "version": "6a4bc757dc7f536b1ff12965462bc10eb7e03fb3f1d3a4c93284b35cfc4eb488"}, "ensemble.GradientBoostingClassifierBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "ensemble.GradientBoostingClassifierBenchmark.peakmem_fit", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "ensemble:55", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "cc97dee1993c6d12e78438319cd9469d26f81a43f7938e1c6008b3953a2bd7ad"}, "ensemble.GradientBoostingClassifierBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "ensemble.GradientBoostingClassifierBenchmark.peakmem_predict", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "ensemble:55", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "5f33dca7623ebcf5b9da857840f38d4160d0eefbdfaaae93a0df0eff427c1d84"}, "ensemble.GradientBoostingClassifierBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "ensemble.GradientBoostingClassifierBenchmark.time_fit", "number": 0, "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "ensemble:55", "timeout": 500, "type": "time", "unit": "seconds", "version": "0d76a561d7d93f238535f79890799de6081b98af1b48172ec072d9f35a80c556", "warmup_time": 1}, "ensemble.GradientBoostingClassifierBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "ensemble.GradientBoostingClassifierBenchmark.time_predict", "number": 0, "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "ensemble:55", "timeout": 500, "type": "time", "unit": "seconds", "version": "07e35a379ff33d5642a8e688571b88abaa50f49959b22447600813915b18f5f5", "warmup_time": 1}, "ensemble.GradientBoostingClassifierBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "ensemble.GradientBoostingClassifierBenchmark.track_test_score", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "ensemble:55", "timeout": 500, "type": "track", "unit": "unit", "version": "5aa3692fb65a37ed9286c6ea97af19db99c401629673d6ebcb9a847fb0f50b3d"}, "ensemble.GradientBoostingClassifierBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "ensemble.GradientBoostingClassifierBenchmark.track_train_score", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "ensemble:55", "timeout": 500, "type": "track", "unit": "unit", "version": "61a34f67a5943b1efa1968705ece7be5cd9e97fba27bc7a42ca9a5e61f07eb52"}, "ensemble.RandomForestClassifierBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "ensemble.RandomForestClassifierBenchmark.peakmem_fit", "param_names": ["representation", "n_jobs"], "params": [["'dense'", "'sparse'"], ["1", "4"]], "setup_cache_key": "ensemble:17", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "55af394470ec69b30129fd9983d4402432d1148dfe0d1c5e8fe57a066313c5bc"}, "ensemble.RandomForestClassifierBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "ensemble.RandomForestClassifierBenchmark.peakmem_predict", "param_names": ["representation", "n_jobs"], "params": [["'dense'", "'sparse'"], ["1", "4"]], "setup_cache_key": "ensemble:17", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "e92c3b611bdc16c763e9fc9210e3850aa3ae5b7230677c2f03a6e1e267ca41c5"}, "ensemble.RandomForestClassifierBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "ensemble.RandomForestClassifierBenchmark.time_fit", "number": 0, "param_names": ["representation", "n_jobs"], "params": [["'dense'", "'sparse'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "ensemble:17", "timeout": 500, "type": "time", "unit": "seconds", "version": "1c07b253be62fce0a4828c260541f370c60bf286df1079e5a868f1597a7ba403", "warmup_time": 1}, "ensemble.RandomForestClassifierBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "ensemble.RandomForestClassifierBenchmark.time_predict", "number": 0, "param_names": ["representation", "n_jobs"], "params": [["'dense'", "'sparse'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "ensemble:17", "timeout": 500, "type": "time", "unit": "seconds", "version": "336d398a53cc6c24a3f37b2493ede95c50fd9bae5093bb07b8bb6f044324dfdd", "warmup_time": 1}, "ensemble.RandomForestClassifierBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "ensemble.RandomForestClassifierBenchmark.track_test_score", "param_names": ["representation", "n_jobs"], "params": [["'dense'", "'sparse'"], ["1", "4"]], "setup_cache_key": "ensemble:17", "timeout": 500, "type": "track", "unit": "unit", "version": "e14fd390f6da53ec6b8c47fb1eee7139ed1b66de7f6491e051941aac2dd3b416"}, "ensemble.RandomForestClassifierBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "ensemble.RandomForestClassifierBenchmark.track_train_score", "param_names": ["representation", "n_jobs"], "params": [["'dense'", "'sparse'"], ["1", "4"]], "setup_cache_key": "ensemble:17", "timeout": 500, "type": "track", "unit": "unit", "version": "e7cd145fca061e4cc80f6e9b17a7beb2465360102c477407540fe4b86bf129a4"}, "linear_model.ElasticNetBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.ElasticNetBenchmark.peakmem_fit", "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "setup_cache_key": "linear_model:175", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "28c73acbc0d080902bd2724f4acba17ae53a5d446c766f31c8f7d6b4bcdf795a"}, "linear_model.ElasticNetBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.ElasticNetBenchmark.peakmem_predict", "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "setup_cache_key": "linear_model:175", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "f15eca10627e9e0918ddceeaa1a7ec5b66c48d8267d639d16cd7cca7c845ce3b"}, "linear_model.ElasticNetBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.ElasticNetBenchmark.time_fit", "number": 0, "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:175", "timeout": 500, "type": "time", "unit": "seconds", "version": "51ff64daecf6ce6464125feebc7b0f40fb4ac09f6ae76a51c92b7c877ae4dabe", "warmup_time": 1}, "linear_model.ElasticNetBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.ElasticNetBenchmark.time_predict", "number": 0, "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:175", "timeout": 500, "type": "time", "unit": "seconds", "version": "91b770a9ba1111b5b98d34b3ec3b6e4272bfb7973c727b829eff4898b1317e6f", "warmup_time": 1}, "linear_model.ElasticNetBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.ElasticNetBenchmark.track_test_score", "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "setup_cache_key": "linear_model:175", "timeout": 500, "type": "track", "unit": "unit", "version": "32f80b46b3e70b4627de35a21f0a5891e2a74b77490a980b894f315f8f8a23a7"}, "linear_model.ElasticNetBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.ElasticNetBenchmark.track_train_score", "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "setup_cache_key": "linear_model:175", "timeout": 500, "type": "track", "unit": "unit", "version": "9d8f3476eeeb0674e29f4482a82870ef1c00638cc9294584483e6b1a0505a07d"}, "linear_model.LassoBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LassoBenchmark.peakmem_fit", "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "setup_cache_key": "linear_model:218", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "b0dadfa535e73bb2d13461ca245f512953fbb93f299bc95c3459302652b07d6b"}, "linear_model.LassoBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LassoBenchmark.peakmem_predict", "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "setup_cache_key": "linear_model:218", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "731893e0b12c495801aa4592439c25dfa6e134813acaf875d0223d967a2cb844"}, "linear_model.LassoBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.LassoBenchmark.time_fit", "number": 0, "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:218", "timeout": 500, "type": "time", "unit": "seconds", "version": "a0883a2adc4f493a8127a070e4bf142445948779c7cdddbdcd6d163259d20796", "warmup_time": 1}, "linear_model.LassoBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.LassoBenchmark.time_predict", "number": 0, "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:218", "timeout": 500, "type": "time", "unit": "seconds", "version": "08cca0bd03f7ac27ebdeb7e683e362c63a114b746b4ea44149df04f08441812e", "warmup_time": 1}, "linear_model.LassoBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LassoBenchmark.track_test_score", "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "setup_cache_key": "linear_model:218", "timeout": 500, "type": "track", "unit": "unit", "version": "2cb69e90fe3348aa8163d5702f21dc704e90321c3193fd397e25cb9a3a80f2c4"}, "linear_model.LassoBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LassoBenchmark.track_train_score", "param_names": ["representation", "precompute"], "params": [["'dense'", "'sparse'"], ["True", "False"]], "setup_cache_key": "linear_model:218", "timeout": 500, "type": "track", "unit": "unit", "version": "530ae6319b705e1b7b9c8653b28f83d6d7556309c83fedffe99aa23a8ca63633"}, "linear_model.LinearRegressionBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LinearRegressionBenchmark.peakmem_fit", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "linear_model:109", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "fded6cdb7544c9b687b7393fc5ade04b42d1b6150d4412708c27ee97037338be"}, "linear_model.LinearRegressionBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LinearRegressionBenchmark.peakmem_predict", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "linear_model:109", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "7e91327ad48684b6bc6161727401f32e004fe4ded945a7e7d9a9320a370ceaa7"}, "linear_model.LinearRegressionBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.LinearRegressionBenchmark.time_fit", "number": 0, "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:109", "timeout": 500, "type": "time", "unit": "seconds", "version": "b16188fca72aa0564f45e39b075510d1e2afa784e3ad92a1bd2c97dbbdd8c7dd", "warmup_time": 1}, "linear_model.LinearRegressionBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.LinearRegressionBenchmark.time_predict", "number": 0, "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:109", "timeout": 500, "type": "time", "unit": "seconds", "version": "bd7cb5a02ebdd3cfc557cdcaa7d2d4ce386faac4ef90b850c55df3cf2ec6acbc", "warmup_time": 1}, "linear_model.LinearRegressionBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LinearRegressionBenchmark.track_test_score", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "linear_model:109", "timeout": 500, "type": "track", "unit": "unit", "version": "cca172d913657c29eab5cb151a16a436bcbce59f2fa0b45f974a4f81a20ea922"}, "linear_model.LinearRegressionBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LinearRegressionBenchmark.track_train_score", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "linear_model:109", "timeout": 500, "type": "track", "unit": "unit", "version": "e328cc796a311efd30fd79e4fd9872c08320937dca931c4071d34608d6fec18f"}, "linear_model.LogisticRegressionBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LogisticRegressionBenchmark.peakmem_fit", "param_names": ["representation", "solver", "n_jobs"], "params": [["'dense'", "'sparse'"], ["'lbfgs'", "'saga'"], ["1", "4"]], "setup_cache_key": "linear_model:20", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "227b880c2ee8b9c108b7a50950b3a603b2dc40e4cabc7cb4477855a83667b09d"}, "linear_model.LogisticRegressionBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LogisticRegressionBenchmark.peakmem_predict", "param_names": ["representation", "solver", "n_jobs"], "params": [["'dense'", "'sparse'"], ["'lbfgs'", "'saga'"], ["1", "4"]], "setup_cache_key": "linear_model:20", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "df044eb5a9c17b37de661eaef6893198a3487e81520a885b94ef03240ef8837b"}, "linear_model.LogisticRegressionBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.LogisticRegressionBenchmark.time_fit", "number": 0, "param_names": ["representation", "solver", "n_jobs"], "params": [["'dense'", "'sparse'"], ["'lbfgs'", "'saga'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:20", "timeout": 500, "type": "time", "unit": "seconds", "version": "3d4beec90f283c2cbd19a02606f3f7854814c69806ea71019959604a4d39a9d3", "warmup_time": 1}, "linear_model.LogisticRegressionBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.LogisticRegressionBenchmark.time_predict", "number": 0, "param_names": ["representation", "solver", "n_jobs"], "params": [["'dense'", "'sparse'"], ["'lbfgs'", "'saga'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:20", "timeout": 500, "type": "time", "unit": "seconds", "version": "6f2562b5f8d268cf7055e64b940013860706e65b134f84e32339f5875723eda2", "warmup_time": 1}, "linear_model.LogisticRegressionBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LogisticRegressionBenchmark.track_test_score", "param_names": ["representation", "solver", "n_jobs"], "params": [["'dense'", "'sparse'"], ["'lbfgs'", "'saga'"], ["1", "4"]], "setup_cache_key": "linear_model:20", "timeout": 500, "type": "track", "unit": "unit", "version": "3188fa6ed863c4941526dc4155a91de5770e1323ec73aa4a230cec942097b2d5"}, "linear_model.LogisticRegressionBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.LogisticRegressionBenchmark.track_train_score", "param_names": ["representation", "solver", "n_jobs"], "params": [["'dense'", "'sparse'"], ["'lbfgs'", "'saga'"], ["1", "4"]], "setup_cache_key": "linear_model:20", "timeout": 500, "type": "track", "unit": "unit", "version": "8eda4a42bdf21139b9ebe1f71eacdac69f314f14a2d35caacbba7286481392c2"}, "linear_model.RidgeBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.RidgeBenchmark.peakmem_fit", "param_names": ["representation", "solver"], "params": [["'dense'", "'sparse'"], ["'auto'", "'svd'", "'cholesky'", "'lsqr'", "'sparse_cg'", "'sag'", "'saga'"]], "setup_cache_key": "linear_model:66", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "0fb7961b46be1655cff5ec1cc08fc925fd173710418d9a31fb5b0a56b428303d"}, "linear_model.RidgeBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.RidgeBenchmark.peakmem_predict", "param_names": ["representation", "solver"], "params": [["'dense'", "'sparse'"], ["'auto'", "'svd'", "'cholesky'", "'lsqr'", "'sparse_cg'", "'sag'", "'saga'"]], "setup_cache_key": "linear_model:66", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "919b8ad2aa4dce054f744439add082932e11625a2bb6f274a5f5c190f5d74ee0"}, "linear_model.RidgeBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.RidgeBenchmark.time_fit", "number": 0, "param_names": ["representation", "solver"], "params": [["'dense'", "'sparse'"], ["'auto'", "'svd'", "'cholesky'", "'lsqr'", "'sparse_cg'", "'sag'", "'saga'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:66", "timeout": 500, "type": "time", "unit": "seconds", "version": "8a363dc27cdb675a4c9fc87ded4ffc43d0d06b9e73e786cb2d945c7c170917e5", "warmup_time": 1}, "linear_model.RidgeBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.RidgeBenchmark.time_predict", "number": 0, "param_names": ["representation", "solver"], "params": [["'dense'", "'sparse'"], ["'auto'", "'svd'", "'cholesky'", "'lsqr'", "'sparse_cg'", "'sag'", "'saga'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:66", "timeout": 500, "type": "time", "unit": "seconds", "version": "6c702677952dfcf9626c6abec5f29d6cb4d5f7c833e6d66d0167375a17d8f911", "warmup_time": 1}, "linear_model.RidgeBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.RidgeBenchmark.track_test_score", "param_names": ["representation", "solver"], "params": [["'dense'", "'sparse'"], ["'auto'", "'svd'", "'cholesky'", "'lsqr'", "'sparse_cg'", "'sag'", "'saga'"]], "setup_cache_key": "linear_model:66", "timeout": 500, "type": "track", "unit": "unit", "version": "b1e2308ac9f2bfe08840f9c17838fd74aef28ffaf207ed8c20d96bdb6cb2b58f"}, "linear_model.RidgeBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.RidgeBenchmark.track_train_score", "param_names": ["representation", "solver"], "params": [["'dense'", "'sparse'"], ["'auto'", "'svd'", "'cholesky'", "'lsqr'", "'sparse_cg'", "'sag'", "'saga'"]], "setup_cache_key": "linear_model:66", "timeout": 500, "type": "track", "unit": "unit", "version": "e96065aa177f3ef7660e8b1c5d33903d198a160073bb32bae3bf31859fb790e2"}, "linear_model.SGDRegressorBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.SGDRegressorBenchmark.peakmem_fit", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "linear_model:141", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "68b98e0f1064163a34bc0578aa1cec92de65faceac2c895a43dc0fe28f277183"}, "linear_model.SGDRegressorBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.SGDRegressorBenchmark.peakmem_predict", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "linear_model:141", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "813f1a9f4e78a8f537c3da3011432175c068f46d5f99e084797f793bb2ce0c71"}, "linear_model.SGDRegressorBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.SGDRegressorBenchmark.time_fit", "number": 0, "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:141", "timeout": 500, "type": "time", "unit": "seconds", "version": "0746a9008d18b05e045f4be5c1fc1495f4ff577234636923a65172909250f277", "warmup_time": 1}, "linear_model.SGDRegressorBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "linear_model.SGDRegressorBenchmark.time_predict", "number": 0, "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "linear_model:141", "timeout": 500, "type": "time", "unit": "seconds", "version": "92c45304d728e9fc68e0ad075acfab2527c012c257f1138fdc90becf644bddce", "warmup_time": 1}, "linear_model.SGDRegressorBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.SGDRegressorBenchmark.track_test_score", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "linear_model:141", "timeout": 500, "type": "track", "unit": "unit", "version": "e15ee5db5de8aa8755ff4c59edd3ebea7317e15e47b4b960e8af6644fed1eed9"}, "linear_model.SGDRegressorBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "linear_model.SGDRegressorBenchmark.track_train_score", "param_names": ["representation"], "params": [["'dense'", "'sparse'"]], "setup_cache_key": "linear_model:141", "timeout": 500, "type": "track", "unit": "unit", "version": "7732977fb570e6e6e5cc0fe2b8270dc2ce9ee4e0c50fac00d102e91072f3fcf3"}, "manifold.TSNEBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass TSNEBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "manifold.TSNEBenchmark.peakmem_fit", "param_names": ["method"], "params": [["'exact'", "'barnes_hut'"]], "setup_cache_key": "manifold:15", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "68f2ed8a98bbd0198abf7ccece495dede3db25c40073344d4ff26a8921d25d09"}, "manifold.TSNEBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass TSNEBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "manifold.TSNEBenchmark.time_fit", "number": 0, "param_names": ["method"], "params": [["'exact'", "'barnes_hut'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "manifold:15", "timeout": 500, "type": "time", "unit": "seconds", "version": "0bc09923597164ea9b4351cdbede223850c7012f81b646429e3637ea2b4fc222", "warmup_time": 1}, "manifold.TSNEBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass TSNEBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "manifold.TSNEBenchmark.track_test_score", "param_names": ["method"], "params": [["'exact'", "'barnes_hut'"]], "setup_cache_key": "manifold:15", "timeout": 500, "type": "track", "unit": "unit", "version": "082788471a14e89a238c8e6f4bb3d5ea740773a3474b446f642868f9bad812c2"}, "manifold.TSNEBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass TSNEBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "manifold.TSNEBenchmark.track_train_score", "param_names": ["method"], "params": [["'exact'", "'barnes_hut'"]], "setup_cache_key": "manifold:15", "timeout": 500, "type": "track", "unit": "unit", "version": "ea38ad09ca9102953d5956c98df7dc58fbd07b38685c6d31ce41db07c81c00b7"}, "metrics.PairwiseDistancesBenchmark.peakmem_pairwise_distances": {"code": "class PairwiseDistancesBenchmark:\n    def peakmem_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == 'sparse' and metric == 'correlation':\n            raise NotImplementedError\n    \n        if Benchmark.data_size == 'large':\n            if metric in ('manhattan', 'correlation'):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in ('manhattan', 'correlation'):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples,\n                               representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {'metric': metric,\n                             'n_jobs': n_jobs}", "name": "metrics.PairwiseDistancesBenchmark.peakmem_pairwise_distances", "param_names": ["representation", "metric", "n_jobs"], "params": [["'dense'", "'sparse'"], ["'cosine'", "'euclidean'", "'manhattan'", "'correlation'"], ["1", "4"]], "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "9e233ad5c2450fe03927e9610c457ce1c967b83279fe878e2280101e3e1ae80d"}, "metrics.PairwiseDistancesBenchmark.time_pairwise_distances": {"code": "class PairwiseDistancesBenchmark:\n    def time_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == 'sparse' and metric == 'correlation':\n            raise NotImplementedError\n    \n        if Benchmark.data_size == 'large':\n            if metric in ('manhattan', 'correlation'):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in ('manhattan', 'correlation'):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples,\n                               representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {'metric': metric,\n                             'n_jobs': n_jobs}", "min_run_count": 2, "name": "metrics.PairwiseDistancesBenchmark.time_pairwise_distances", "number": 0, "param_names": ["representation", "metric", "n_jobs"], "params": [["'dense'", "'sparse'"], ["'cosine'", "'euclidean'", "'manhattan'", "'correlation'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "timeout": 500, "type": "time", "unit": "seconds", "version": "9fed496f3dad2c9dc76f028f417784b342266ee16197672e899e403a5447b57c", "warmup_time": 1}, "model_selection.CrossValidationBenchmark.peakmem_crossval": {"code": "class CrossValidationBenchmark:\n    def peakmem_crossval(self, *args):\n        cross_val_score(self.clf, self.X, self.y, **self.cv_params)\n\n    def setup(self, *params):\n        n_jobs, = params\n    \n        data = _synth_classification_dataset(n_samples=50000, n_features=100)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.clf = RandomForestClassifier(n_estimators=50,\n                                          max_depth=10,\n                                          random_state=0)\n    \n        cv = 16 if Benchmark.data_size == 'large' else 4\n    \n        self.cv_params = {'n_jobs': n_jobs,\n                          'cv': cv}", "name": "model_selection.CrossValidationBenchmark.peakmem_crossval", "param_names": ["n_jobs"], "params": [["1", "4"]], "timeout": 20000, "type": "peakmemory", "unit": "bytes", "version": "6801caf1f8cd21ed17a0095ee8a6a4f32e18fc9c4fda5efc812cd8c88053146b"}, "model_selection.CrossValidationBenchmark.time_crossval": {"code": "class CrossValidationBenchmark:\n    def time_crossval(self, *args):\n        cross_val_score(self.clf, self.X, self.y, **self.cv_params)\n\n    def setup(self, *params):\n        n_jobs, = params\n    \n        data = _synth_classification_dataset(n_samples=50000, n_features=100)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.clf = RandomForestClassifier(n_estimators=50,\n                                          max_depth=10,\n                                          random_state=0)\n    \n        cv = 16 if Benchmark.data_size == 'large' else 4\n    \n        self.cv_params = {'n_jobs': n_jobs,\n                          'cv': cv}", "min_run_count": 2, "name": "model_selection.CrossValidationBenchmark.time_crossval", "number": 0, "param_names": ["n_jobs"], "params": [["1", "4"]], "rounds": 1, "sample_time": 0.01, "timeout": 20000, "type": "time", "unit": "seconds", "version": "be3520243b65d79e9c474fd2297b14127cf9de93ebc3c36d5cbf473e515a7f51", "warmup_time": 1}, "model_selection.CrossValidationBenchmark.track_crossval": {"code": "class CrossValidationBenchmark:\n    def track_crossval(self, *args):\n        return float(cross_val_score(self.clf, self.X,\n                                     self.y, **self.cv_params).mean())\n\n    def setup(self, *params):\n        n_jobs, = params\n    \n        data = _synth_classification_dataset(n_samples=50000, n_features=100)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.clf = RandomForestClassifier(n_estimators=50,\n                                          max_depth=10,\n                                          random_state=0)\n    \n        cv = 16 if Benchmark.data_size == 'large' else 4\n    \n        self.cv_params = {'n_jobs': n_jobs,\n                          'cv': cv}", "name": "model_selection.CrossValidationBenchmark.track_crossval", "param_names": ["n_jobs"], "params": [["1", "4"]], "timeout": 20000, "type": "track", "unit": "unit", "version": "74d326542acd7965a4ac38cd7dcbf848a4ee175d5aee479001a1e7846287e113"}, "model_selection.GridSearchBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "model_selection.GridSearchBenchmark.peakmem_fit", "param_names": ["n_jobs"], "params": [["1", "4"]], "setup_cache_key": "model_selection:55", "timeout": 20000, "type": "peakmemory", "unit": "bytes", "version": "d2dd984f5f9ffe61ece803d4ee2f05375513c546c7de765e24ec1edff3359d75"}, "model_selection.GridSearchBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "model_selection.GridSearchBenchmark.peakmem_predict", "param_names": ["n_jobs"], "params": [["1", "4"]], "setup_cache_key": "model_selection:55", "timeout": 20000, "type": "peakmemory", "unit": "bytes", "version": "0a181b0ce7fbc54bf6b7032db6b39562542ddef6a35255668b9a088799e225c3"}, "model_selection.GridSearchBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "model_selection.GridSearchBenchmark.time_fit", "number": 0, "param_names": ["n_jobs"], "params": [["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "model_selection:55", "timeout": 20000, "type": "time", "unit": "seconds", "version": "6e8041bfb2a873d368978c028fbe561790c06c5a0d0ec5e6110543525f5cb9a2", "warmup_time": 1}, "model_selection.GridSearchBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "model_selection.GridSearchBenchmark.time_predict", "number": 0, "param_names": ["n_jobs"], "params": [["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "model_selection:55", "timeout": 20000, "type": "time", "unit": "seconds", "version": "dcdea000aea14892ba793d2354b387d4a23d187fcdc6d32e167d544ba4e2b0cc", "warmup_time": 1}, "model_selection.GridSearchBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "model_selection.GridSearchBenchmark.track_test_score", "param_names": ["n_jobs"], "params": [["1", "4"]], "setup_cache_key": "model_selection:55", "timeout": 20000, "type": "track", "unit": "unit", "version": "6469d2e25b08248849a365b43fd7c9f442201399d7a690e1962a95df3b39722f"}, "model_selection.GridSearchBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "model_selection.GridSearchBenchmark.track_train_score", "param_names": ["n_jobs"], "params": [["1", "4"]], "setup_cache_key": "model_selection:55", "timeout": 20000, "type": "track", "unit": "unit", "version": "e3c89e4a58c0cae0a5999ef85ae2809732335d78588dd2e02b3005e5535606fb"}, "neighbors.KNeighborsClassifierBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "neighbors.KNeighborsClassifierBenchmark.peakmem_fit", "param_names": ["algorithm", "dimension", "n_jobs"], "params": [["'brute'", "'kd_tree'", "'ball_tree'"], ["'low'", "'high'"], ["1", "4"]], "setup_cache_key": "neighbors:18", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "43360d17ecddfc1659b798464839c3bac0e83d375aa1053cf4cbc45f2107f868"}, "neighbors.KNeighborsClassifierBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "neighbors.KNeighborsClassifierBenchmark.peakmem_predict", "param_names": ["algorithm", "dimension", "n_jobs"], "params": [["'brute'", "'kd_tree'", "'ball_tree'"], ["'low'", "'high'"], ["1", "4"]], "setup_cache_key": "neighbors:18", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "a9b867976f10c5b9de77e884b025c937079c4736d48a0a916aafb215ccd5c6b5"}, "neighbors.KNeighborsClassifierBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "neighbors.KNeighborsClassifierBenchmark.time_fit", "number": 0, "param_names": ["algorithm", "dimension", "n_jobs"], "params": [["'brute'", "'kd_tree'", "'ball_tree'"], ["'low'", "'high'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "neighbors:18", "timeout": 500, "type": "time", "unit": "seconds", "version": "b69699ce3d079dbb3cdb110c8538804c4b1121ef6f38f473979b0063bbd0c42b", "warmup_time": 1}, "neighbors.KNeighborsClassifierBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "neighbors.KNeighborsClassifierBenchmark.time_predict", "number": 0, "param_names": ["algorithm", "dimension", "n_jobs"], "params": [["'brute'", "'kd_tree'", "'ball_tree'"], ["'low'", "'high'"], ["1", "4"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "neighbors:18", "timeout": 500, "type": "time", "unit": "seconds", "version": "e855c72d2ecca559cf5a5b0281ee724eaffc87fd376347c28ae6d9007b7889a4", "warmup_time": 1}, "neighbors.KNeighborsClassifierBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "neighbors.KNeighborsClassifierBenchmark.track_test_score", "param_names": ["algorithm", "dimension", "n_jobs"], "params": [["'brute'", "'kd_tree'", "'ball_tree'"], ["'low'", "'high'"], ["1", "4"]], "setup_cache_key": "neighbors:18", "timeout": 500, "type": "track", "unit": "unit", "version": "7579889d2fe88fa901e741f40636a3145f8df3dc88ddbc26962d150f48a68a80"}, "neighbors.KNeighborsClassifierBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "neighbors.KNeighborsClassifierBenchmark.track_train_score", "param_names": ["algorithm", "dimension", "n_jobs"], "params": [["'brute'", "'kd_tree'", "'ball_tree'"], ["'low'", "'high'"], ["1", "4"]], "setup_cache_key": "neighbors:18", "timeout": 500, "type": "track", "unit": "unit", "version": "08d43de9f99e9a3f1640453df03be3a7dd66740bc8ad71de8f6931c856f9815f"}, "svm.SVCBenchmark.peakmem_fit": {"code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "svm.SVCBenchmark.peakmem_fit", "param_names": ["kernel"], "params": [["'linear'", "'poly'", "'rbf'", "'sigmoid'"]], "setup_cache_key": "svm:14", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "d61868e19dbb13fb2fe3bf053eb8b41c8a099a959f559ff9ca2d4e96639488ec"}, "svm.SVCBenchmark.peakmem_predict": {"code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "svm.SVCBenchmark.peakmem_predict", "param_names": ["kernel"], "params": [["'linear'", "'poly'", "'rbf'", "'sigmoid'"]], "setup_cache_key": "svm:14", "timeout": 500, "type": "peakmemory", "unit": "bytes", "version": "7bf277d2275581194a28809fe59ba881f789d3251b6934680957c18ec833bed8"}, "svm.SVCBenchmark.time_fit": {"code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "svm.SVCBenchmark.time_fit", "number": 0, "param_names": ["kernel"], "params": [["'linear'", "'poly'", "'rbf'", "'sigmoid'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "svm:14", "timeout": 500, "type": "time", "unit": "seconds", "version": "87b16eef694e92177abd53b94cf22434f4d1d2b24472384edbba12f3ffd79e5d", "warmup_time": 1}, "svm.SVCBenchmark.time_predict": {"code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "min_run_count": 2, "name": "svm.SVCBenchmark.time_predict", "number": 0, "param_names": ["kernel"], "params": [["'linear'", "'poly'", "'rbf'", "'sigmoid'"]], "rounds": 1, "sample_time": 0.01, "setup_cache_key": "svm:14", "timeout": 500, "type": "time", "unit": "seconds", "version": "008eecb5025ad842ac7693131008e28a6fedfb423560d9692b596b7cb69f905a", "warmup_time": 1}, "svm.SVCBenchmark.track_test_score": {"code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "svm.SVCBenchmark.track_test_score", "param_names": ["kernel"], "params": [["'linear'", "'poly'", "'rbf'", "'sigmoid'"]], "setup_cache_key": "svm:14", "timeout": 500, "type": "track", "unit": "unit", "version": "9bfdd3bfb929bf4d6c394a4e6b3e766d2887a161011ebeff3a32e20d4f46dc98"}, "svm.SVCBenchmark.track_train_score": {"code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()", "name": "svm.SVCBenchmark.track_train_score", "param_names": ["kernel"], "params": [["'linear'", "'poly'", "'rbf'", "'sigmoid'"]], "setup_cache_key": "svm:14", "timeout": 500, "type": "track", "unit": "unit", "version": "e43292f2ef23d0a9c445b7e69128d96e90ea2522bdd8858eae0051bc20d54385"}}, "machines": {"sklearn-benchmark": {"arch": "x86_64", "cpu": "Intel Core Processor (Haswell, no TSX)", "machine": "sklearn-benchmark", "num_cpu": "8", "os": "Linux 4.15.0-20-generic", "ram": "16424684", "version": 1}}, "tags": {"0.1": 395, "0.1-beta": 382, "0.10": 7904, "0.10-branching": 7872, "0.11": 9357, "0.11-beta": 9331, "0.11-branching": 9349, "0.12": 10436, "0.12-branching": 10413, "0.12.1": 10778, "0.13": 12373, "0.13-branching": 12368, "0.13.1": 12748, "0.14": 14698, "0.14.1": 14725, "0.14a1": 14515, "0.15-branching": 17074, "0.15.0": 17279, "0.15.0b1": 16940, "0.15.0b2": 17075, "0.15.1": 17645, "0.15.2": 17934, "0.16-branching": 19198, "0.16.0": 19375, "0.16.1": 19504, "0.16b1": 19199, "0.17": 21113, "0.17-branching": 20502, "0.17.1": 21601, "0.17.1-1": 21602, "0.17b1": 20509, "0.18": 22393, "0.18.1": 22702, "0.18.2": 23197, "0.18rc": 22265, "0.18rc1": 22268, "0.18rc2": 22283, "0.19-branching": 23280, "0.19.0": 23480, "0.19.1": 23768, "0.19.2": 24417, "0.19b1": 23283, "0.19b2": 23307, "0.2": 590, "0.2-beta": 587, "0.20.0": 24809, "0.20.1": 25159, "0.20.2": 25261, "0.20.3": 25563, "0.20.4": 26284, "0.20rc1": 24644, "0.21.0": 25841, "0.21.1": 25874, "0.21.2": 25910, "0.21.3": 26278, "0.21b2": 25766, "0.21rc1": 25758, "0.21rc2": 25767, "0.22": 26953, "0.22.1": 27063, "0.22.2": 27300, "0.22.2.post1": 27325, "0.22rc1": 26820, "0.22rc2": 26881, "0.22rc2.post1": 26890, "0.22rc3": 26944, "0.23.0": 27607, "0.23.0rc1": 27574, "0.23.1": 27677, "0.23.2": 28089, "0.3": 745, "0.4": 998, "0.5": 1969, "0.5.rc": 1915, "0.5.rc2": 1917, "0.5.rc3": 1919, "0.6-rc": 2701, "0.6.0": 2711, "0.7": 3151, "0.7-branching": 3102, "0.7.1": 3212, "0.8": 4037, "0.8-branching": 3905, "0.8.1": 4684, "0.9": 6225, "0.9-branching": 6177, "debian/0.10.0-1": 7919, "debian/0.11.0-1": 9369, "debian/0.11.0-2": 9799, "debian/0.12.0-1": 10457, "debian/0.16.1-2": 19920, "debian/0.17.0-1": 21126, "debian/0.17.0-3": 21322, "debian/0.17.0-4": 21323, "debian/0.17.0_b1+git14-g4e6829c-1": 20955, "debian/0.17.0_b1-1": 20952, "debian/0.2+svn625-1": 646, "debian/0.3-1": 755, "debian/0.3-2": 811, "debian/0.3-3": 813, "debian/0.3-4": 826, "debian/0.4-1": 1016, "debian/0.4-2": 1131, "debian/0.4-3": 1288, "debian/0.5-1": 1977, "debian/0.6.0.dfsg-1": 2743, "debian/0.7.1.dfsg-1": 3289, "debian/0.7.1.dfsg-3": 3741, "debian/0.8.0.dfsg-1": 4054, "debian/0.8.1.dfsg-1": 4696, "debian/0.9.0.dfsg-1": 6511, "sprint01": 1207}, "pages": [["", "Grid view", "Display as a agrid"], ["summarylist", "List view", "Display as a list"], ["regressions", "Show regressions", "Display information about recent regressions"]]}