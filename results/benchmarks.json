{
    "cluster.KMeansBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "cluster.KMeansBenchmark.peakmem_fit",
        "param_names": [
            "representation",
            "algorithm",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'full'",
                "'elkan'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "setup_cache_key": "cluster:16",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "44996b30a90296496e9fbd453654a32aa6c2909019acefc1ecae8d8eb4609d15"
    },
    "cluster.KMeansBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "cluster.KMeansBenchmark.peakmem_predict",
        "param_names": [
            "representation",
            "algorithm",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'full'",
                "'elkan'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "setup_cache_key": "cluster:16",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "510a8c6e9aad0a40d0f8a0b5e80a5e07f0d8151c5efce1d37b2fc63c39bb32e5"
    },
    "cluster.KMeansBenchmark.peakmem_transform": {
        "code": "class Transformer:\n    def peakmem_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "cluster.KMeansBenchmark.peakmem_transform",
        "param_names": [
            "representation",
            "algorithm",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'full'",
                "'elkan'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "setup_cache_key": "cluster:16",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "70a2cb80848550589a4507ca5a90210a9df9621d3fbe9a665f3fe215d263ac25"
    },
    "cluster.KMeansBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "cluster.KMeansBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "representation",
            "algorithm",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'full'",
                "'elkan'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "cluster:16",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "a7f0a87fd088d4fcaf9d60644c6c653c75d1261c10be350e2ad325cbbade2aaf",
        "warmup_time": 1
    },
    "cluster.KMeansBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "cluster.KMeansBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "representation",
            "algorithm",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'full'",
                "'elkan'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "cluster:16",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "75e2bf5cab4c55598df2a5964235bb56158f1fd58fbe88037bbeb27518941430",
        "warmup_time": 1
    },
    "cluster.KMeansBenchmark.time_transform": {
        "code": "class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "cluster.KMeansBenchmark.time_transform",
        "number": 0,
        "param_names": [
            "representation",
            "algorithm",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'full'",
                "'elkan'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "cluster:16",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "e89a4fa90042114d4904732525795519e353e717562e734ef51e1a17fdb37ee2",
        "warmup_time": 1
    },
    "cluster.KMeansBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "cluster.KMeansBenchmark.track_test_score",
        "param_names": [
            "representation",
            "algorithm",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'full'",
                "'elkan'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "setup_cache_key": "cluster:16",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "3c3af2849d342cebc878a7ac97ad167c9db632ac6e72ee902c1f0def6ef488b8"
    },
    "cluster.KMeansBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "cluster.KMeansBenchmark.track_train_score",
        "param_names": [
            "representation",
            "algorithm",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'full'",
                "'elkan'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "setup_cache_key": "cluster:16",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "0b1413310939b67bdbb6490a3b9ccc19d0f4bf68615214df8463596abc27cd29"
    },
    "cluster.MiniBatchKMeansBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "cluster.MiniBatchKMeansBenchmark.peakmem_fit",
        "param_names": [
            "representation",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "setup_cache_key": "cluster:63",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "27a823a52563653b94cee28aee3234581dfdc555a0360637498b08166bc1d9ad"
    },
    "cluster.MiniBatchKMeansBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "cluster.MiniBatchKMeansBenchmark.peakmem_predict",
        "param_names": [
            "representation",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "setup_cache_key": "cluster:63",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "41369a0e02bed31162a08bdf8f5fa2bac1ece3644dd88e977837dc5ebfc9449c"
    },
    "cluster.MiniBatchKMeansBenchmark.peakmem_transform": {
        "code": "class Transformer:\n    def peakmem_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "cluster.MiniBatchKMeansBenchmark.peakmem_transform",
        "param_names": [
            "representation",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "setup_cache_key": "cluster:63",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "3809620b44698eaa55ea358f5867374ef6b4f0dc3b7b3fb2bd28de40db441d22"
    },
    "cluster.MiniBatchKMeansBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "cluster.MiniBatchKMeansBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "representation",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "cluster:63",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "f3fda2b1a621d39ede4bc2103dede0fde92fbd210d39d015ea61934bfccfc7c2",
        "warmup_time": 1
    },
    "cluster.MiniBatchKMeansBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "cluster.MiniBatchKMeansBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "representation",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "cluster:63",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "23ec70517acd7b8eba2cdc3cc9c57604c51b5639b32fffc729a4f20a24b06d72",
        "warmup_time": 1
    },
    "cluster.MiniBatchKMeansBenchmark.time_transform": {
        "code": "class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "cluster.MiniBatchKMeansBenchmark.time_transform",
        "number": 0,
        "param_names": [
            "representation",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "cluster:63",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "80d12f75cd37166bced2570d27f5250389ec6f7df8982e734288778fa0067280",
        "warmup_time": 1
    },
    "cluster.MiniBatchKMeansBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "cluster.MiniBatchKMeansBenchmark.track_test_score",
        "param_names": [
            "representation",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "setup_cache_key": "cluster:63",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "b15e6712b29288eb2cf5fc5bd336054544a9002dfab16a6e299d6f992154edc9"
    },
    "cluster.MiniBatchKMeansBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchKMeansBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "cluster.MiniBatchKMeansBenchmark.track_train_score",
        "param_names": [
            "representation",
            "init"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'random'",
                "'k-means++'"
            ]
        ],
        "setup_cache_key": "cluster:63",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "858db1057ab8f972235963a5c0e46e44c848881c70c927afddb64243168000b0"
    },
    "decomposition.DictionaryLearningBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.DictionaryLearningBenchmark.peakmem_fit",
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "decomposition:44",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "190e4b8153b3607376247c0a64f4c1d8aa385789d8f2eb6e65be2006fc0f21a0"
    },
    "decomposition.DictionaryLearningBenchmark.peakmem_transform": {
        "code": "class Transformer:\n    def peakmem_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.DictionaryLearningBenchmark.peakmem_transform",
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "decomposition:44",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b5d9d222d5acbb1ba622ed388de04f7bc205f57e0ec5112c0792edad34ad712a"
    },
    "decomposition.DictionaryLearningBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "decomposition.DictionaryLearningBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "decomposition:44",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "3b1cb33cf7cae38b9084e69ee8840abd4a0209018cf63db5ae6d22e2915ecc04",
        "warmup_time": 1
    },
    "decomposition.DictionaryLearningBenchmark.time_transform": {
        "code": "class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "decomposition.DictionaryLearningBenchmark.time_transform",
        "number": 0,
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "decomposition:44",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "78044cbc0c8ef76c735aecaaee722df32400100fd621a5cb73d6207aad3736e8",
        "warmup_time": 1
    },
    "decomposition.DictionaryLearningBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.DictionaryLearningBenchmark.track_test_score",
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "decomposition:44",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "dffe185efa679099c300a200d5f1548d7025bb14150cc130ee9594dbca6a08f4"
    },
    "decomposition.DictionaryLearningBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass DictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.DictionaryLearningBenchmark.track_train_score",
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "decomposition:44",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "e132e82fb0e747524994e89713ee6cc7c16e11ac696381769ba0b7a865f809ee"
    },
    "decomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.MiniBatchDictionaryLearningBenchmark.peakmem_fit",
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "decomposition:75",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7413c15d31d1af9d57b047e1eb9793e8fd9442dd4852f9bbf7944520b6cde539"
    },
    "decomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform": {
        "code": "class Transformer:\n    def peakmem_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.MiniBatchDictionaryLearningBenchmark.peakmem_transform",
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "decomposition:75",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9583e478652b4aa7adf26052264d261ed51218bfb02c392a8dd5afa6f0be9067"
    },
    "decomposition.MiniBatchDictionaryLearningBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "decomposition.MiniBatchDictionaryLearningBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "decomposition:75",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "fbedaca37b514fdecdc98bf0d5e99cc18f4e81adafa0501bf2cb62b395cff80b",
        "warmup_time": 1
    },
    "decomposition.MiniBatchDictionaryLearningBenchmark.time_transform": {
        "code": "class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "decomposition.MiniBatchDictionaryLearningBenchmark.time_transform",
        "number": 0,
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "decomposition:75",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "04e8b945e21dc8f7471c0bb5d123098da806565bcd07f0ae5c78c42492669af1",
        "warmup_time": 1
    },
    "decomposition.MiniBatchDictionaryLearningBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.MiniBatchDictionaryLearningBenchmark.track_test_score",
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "decomposition:75",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "3254255804120abf8154e7fb27ecc492ebf763052bc0b660de4ae3579648a704"
    },
    "decomposition.MiniBatchDictionaryLearningBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass MiniBatchDictionaryLearningBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.MiniBatchDictionaryLearningBenchmark.track_train_score",
        "param_names": [
            "fit_algorithm",
            "n_jobs"
        ],
        "params": [
            [
                "'lars'",
                "'cd'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "decomposition:75",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "68f98a3244c8ad2dec8817d68a45e5c8185c6b2487bca85e5302d103e6bdf725"
    },
    "decomposition.PCABenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.PCABenchmark.peakmem_fit",
        "param_names": [
            "svd_solver"
        ],
        "params": [
            [
                "'full'",
                "'arpack'",
                "'randomized'"
            ]
        ],
        "setup_cache_key": "decomposition:17",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "42a67cdbc3312242272375647a4c8921d248f3932a1f45439cd1ee6fb5bfa1ac"
    },
    "decomposition.PCABenchmark.peakmem_transform": {
        "code": "class Transformer:\n    def peakmem_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.PCABenchmark.peakmem_transform",
        "param_names": [
            "svd_solver"
        ],
        "params": [
            [
                "'full'",
                "'arpack'",
                "'randomized'"
            ]
        ],
        "setup_cache_key": "decomposition:17",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7d07701a485134ed88af807935021a90621db5698a9246bc6da7a2088a7f79c2"
    },
    "decomposition.PCABenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "decomposition.PCABenchmark.time_fit",
        "number": 0,
        "param_names": [
            "svd_solver"
        ],
        "params": [
            [
                "'full'",
                "'arpack'",
                "'randomized'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "decomposition:17",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "0cbc79d745e8e837e60d0fb6ce4c28d169a4f3d0374aa4c41c1af06f20fd00ae",
        "warmup_time": 1
    },
    "decomposition.PCABenchmark.time_transform": {
        "code": "class Transformer:\n    def time_transform(self, *args):\n        self.estimator.transform(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "decomposition.PCABenchmark.time_transform",
        "number": 0,
        "param_names": [
            "svd_solver"
        ],
        "params": [
            [
                "'full'",
                "'arpack'",
                "'randomized'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "decomposition:17",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "32433581033fb7c58370c65de59d3bc16af930d20e4b5b1de444aa4cf264b1f1",
        "warmup_time": 1
    },
    "decomposition.PCABenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.PCABenchmark.track_test_score",
        "param_names": [
            "svd_solver"
        ],
        "params": [
            [
                "'full'",
                "'arpack'",
                "'randomized'"
            ]
        ],
        "setup_cache_key": "decomposition:17",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "76c2fa737efe3334a87e39743a537297128e25318011c3e3ffec6d548f265b4c"
    },
    "decomposition.PCABenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass PCABenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "decomposition.PCABenchmark.track_train_score",
        "param_names": [
            "svd_solver"
        ],
        "params": [
            [
                "'full'",
                "'arpack'",
                "'randomized'"
            ]
        ],
        "setup_cache_key": "decomposition:17",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "6a4bc757dc7f536b1ff12965462bc10eb7e03fb3f1d3a4c93284b35cfc4eb488"
    },
    "ensemble.GradientBoostingClassifierBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "ensemble.GradientBoostingClassifierBenchmark.peakmem_fit",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "ensemble:55",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "cc97dee1993c6d12e78438319cd9469d26f81a43f7938e1c6008b3953a2bd7ad"
    },
    "ensemble.GradientBoostingClassifierBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "ensemble.GradientBoostingClassifierBenchmark.peakmem_predict",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "ensemble:55",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "5f33dca7623ebcf5b9da857840f38d4160d0eefbdfaaae93a0df0eff427c1d84"
    },
    "ensemble.GradientBoostingClassifierBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "ensemble.GradientBoostingClassifierBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "ensemble:55",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "0d76a561d7d93f238535f79890799de6081b98af1b48172ec072d9f35a80c556",
        "warmup_time": 1
    },
    "ensemble.GradientBoostingClassifierBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "ensemble.GradientBoostingClassifierBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "ensemble:55",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "07e35a379ff33d5642a8e688571b88abaa50f49959b22447600813915b18f5f5",
        "warmup_time": 1
    },
    "ensemble.GradientBoostingClassifierBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "ensemble.GradientBoostingClassifierBenchmark.track_test_score",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "ensemble:55",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "5aa3692fb65a37ed9286c6ea97af19db99c401629673d6ebcb9a847fb0f50b3d"
    },
    "ensemble.GradientBoostingClassifierBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GradientBoostingClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "ensemble.GradientBoostingClassifierBenchmark.track_train_score",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "ensemble:55",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "61a34f67a5943b1efa1968705ece7be5cd9e97fba27bc7a42ca9a5e61f07eb52"
    },
    "ensemble.RandomForestClassifierBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "ensemble.RandomForestClassifierBenchmark.peakmem_fit",
        "param_names": [
            "representation",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "ensemble:17",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "55af394470ec69b30129fd9983d4402432d1148dfe0d1c5e8fe57a066313c5bc"
    },
    "ensemble.RandomForestClassifierBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "ensemble.RandomForestClassifierBenchmark.peakmem_predict",
        "param_names": [
            "representation",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "ensemble:17",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "e92c3b611bdc16c763e9fc9210e3850aa3ae5b7230677c2f03a6e1e267ca41c5"
    },
    "ensemble.RandomForestClassifierBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "ensemble.RandomForestClassifierBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "representation",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "ensemble:17",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "1c07b253be62fce0a4828c260541f370c60bf286df1079e5a868f1597a7ba403",
        "warmup_time": 1
    },
    "ensemble.RandomForestClassifierBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "ensemble.RandomForestClassifierBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "representation",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "ensemble:17",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "336d398a53cc6c24a3f37b2493ede95c50fd9bae5093bb07b8bb6f044324dfdd",
        "warmup_time": 1
    },
    "ensemble.RandomForestClassifierBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "ensemble.RandomForestClassifierBenchmark.track_test_score",
        "param_names": [
            "representation",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "ensemble:17",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "e14fd390f6da53ec6b8c47fb1eee7139ed1b66de7f6491e051941aac2dd3b416"
    },
    "ensemble.RandomForestClassifierBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RandomForestClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "ensemble.RandomForestClassifierBenchmark.track_train_score",
        "param_names": [
            "representation",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "ensemble:17",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "e7cd145fca061e4cc80f6e9b17a7beb2465360102c477407540fe4b86bf129a4"
    },
    "linear_model.ElasticNetBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.ElasticNetBenchmark.peakmem_fit",
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "linear_model:175",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "28c73acbc0d080902bd2724f4acba17ae53a5d446c766f31c8f7d6b4bcdf795a"
    },
    "linear_model.ElasticNetBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.ElasticNetBenchmark.peakmem_predict",
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "linear_model:175",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "f15eca10627e9e0918ddceeaa1a7ec5b66c48d8267d639d16cd7cca7c845ce3b"
    },
    "linear_model.ElasticNetBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.ElasticNetBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:175",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "51ff64daecf6ce6464125feebc7b0f40fb4ac09f6ae76a51c92b7c877ae4dabe",
        "warmup_time": 1
    },
    "linear_model.ElasticNetBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.ElasticNetBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:175",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "91b770a9ba1111b5b98d34b3ec3b6e4272bfb7973c727b829eff4898b1317e6f",
        "warmup_time": 1
    },
    "linear_model.ElasticNetBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.ElasticNetBenchmark.track_test_score",
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "linear_model:175",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "32f80b46b3e70b4627de35a21f0a5891e2a74b77490a980b894f315f8f8a23a7"
    },
    "linear_model.ElasticNetBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass ElasticNetBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.ElasticNetBenchmark.track_train_score",
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "linear_model:175",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "9d8f3476eeeb0674e29f4482a82870ef1c00638cc9294584483e6b1a0505a07d"
    },
    "linear_model.LassoBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LassoBenchmark.peakmem_fit",
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "linear_model:218",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "b0dadfa535e73bb2d13461ca245f512953fbb93f299bc95c3459302652b07d6b"
    },
    "linear_model.LassoBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LassoBenchmark.peakmem_predict",
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "linear_model:218",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "731893e0b12c495801aa4592439c25dfa6e134813acaf875d0223d967a2cb844"
    },
    "linear_model.LassoBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.LassoBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:218",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "a0883a2adc4f493a8127a070e4bf142445948779c7cdddbdcd6d163259d20796",
        "warmup_time": 1
    },
    "linear_model.LassoBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.LassoBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:218",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "08cca0bd03f7ac27ebdeb7e683e362c63a114b746b4ea44149df04f08441812e",
        "warmup_time": 1
    },
    "linear_model.LassoBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LassoBenchmark.track_test_score",
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "linear_model:218",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "2cb69e90fe3348aa8163d5702f21dc704e90321c3193fd397e25cb9a3a80f2c4"
    },
    "linear_model.LassoBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LassoBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LassoBenchmark.track_train_score",
        "param_names": [
            "representation",
            "precompute"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "True",
                "False"
            ]
        ],
        "setup_cache_key": "linear_model:218",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "530ae6319b705e1b7b9c8653b28f83d6d7556309c83fedffe99aa23a8ca63633"
    },
    "linear_model.LinearRegressionBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LinearRegressionBenchmark.peakmem_fit",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "linear_model:109",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "fded6cdb7544c9b687b7393fc5ade04b42d1b6150d4412708c27ee97037338be"
    },
    "linear_model.LinearRegressionBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LinearRegressionBenchmark.peakmem_predict",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "linear_model:109",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7e91327ad48684b6bc6161727401f32e004fe4ded945a7e7d9a9320a370ceaa7"
    },
    "linear_model.LinearRegressionBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.LinearRegressionBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:109",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "b16188fca72aa0564f45e39b075510d1e2afa784e3ad92a1bd2c97dbbdd8c7dd",
        "warmup_time": 1
    },
    "linear_model.LinearRegressionBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.LinearRegressionBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:109",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "bd7cb5a02ebdd3cfc557cdcaa7d2d4ce386faac4ef90b850c55df3cf2ec6acbc",
        "warmup_time": 1
    },
    "linear_model.LinearRegressionBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LinearRegressionBenchmark.track_test_score",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "linear_model:109",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "cca172d913657c29eab5cb151a16a436bcbce59f2fa0b45f974a4f81a20ea922"
    },
    "linear_model.LinearRegressionBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LinearRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LinearRegressionBenchmark.track_train_score",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "linear_model:109",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "e328cc796a311efd30fd79e4fd9872c08320937dca931c4071d34608d6fec18f"
    },
    "linear_model.LogisticRegressionBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LogisticRegressionBenchmark.peakmem_fit",
        "param_names": [
            "representation",
            "solver",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'lbfgs'",
                "'saga'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "linear_model:20",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "227b880c2ee8b9c108b7a50950b3a603b2dc40e4cabc7cb4477855a83667b09d"
    },
    "linear_model.LogisticRegressionBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LogisticRegressionBenchmark.peakmem_predict",
        "param_names": [
            "representation",
            "solver",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'lbfgs'",
                "'saga'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "linear_model:20",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "df044eb5a9c17b37de661eaef6893198a3487e81520a885b94ef03240ef8837b"
    },
    "linear_model.LogisticRegressionBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.LogisticRegressionBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "representation",
            "solver",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'lbfgs'",
                "'saga'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:20",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "3d4beec90f283c2cbd19a02606f3f7854814c69806ea71019959604a4d39a9d3",
        "warmup_time": 1
    },
    "linear_model.LogisticRegressionBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.LogisticRegressionBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "representation",
            "solver",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'lbfgs'",
                "'saga'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:20",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "6f2562b5f8d268cf7055e64b940013860706e65b134f84e32339f5875723eda2",
        "warmup_time": 1
    },
    "linear_model.LogisticRegressionBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LogisticRegressionBenchmark.track_test_score",
        "param_names": [
            "representation",
            "solver",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'lbfgs'",
                "'saga'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "linear_model:20",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "3188fa6ed863c4941526dc4155a91de5770e1323ec73aa4a230cec942097b2d5"
    },
    "linear_model.LogisticRegressionBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass LogisticRegressionBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.LogisticRegressionBenchmark.track_train_score",
        "param_names": [
            "representation",
            "solver",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'lbfgs'",
                "'saga'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "linear_model:20",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "8eda4a42bdf21139b9ebe1f71eacdac69f314f14a2d35caacbba7286481392c2"
    },
    "linear_model.RidgeBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.RidgeBenchmark.peakmem_fit",
        "param_names": [
            "representation",
            "solver"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'auto'",
                "'svd'",
                "'cholesky'",
                "'lsqr'",
                "'sparse_cg'",
                "'sag'",
                "'saga'"
            ]
        ],
        "setup_cache_key": "linear_model:66",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0fb7961b46be1655cff5ec1cc08fc925fd173710418d9a31fb5b0a56b428303d"
    },
    "linear_model.RidgeBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.RidgeBenchmark.peakmem_predict",
        "param_names": [
            "representation",
            "solver"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'auto'",
                "'svd'",
                "'cholesky'",
                "'lsqr'",
                "'sparse_cg'",
                "'sag'",
                "'saga'"
            ]
        ],
        "setup_cache_key": "linear_model:66",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "919b8ad2aa4dce054f744439add082932e11625a2bb6f274a5f5c190f5d74ee0"
    },
    "linear_model.RidgeBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.RidgeBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "representation",
            "solver"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'auto'",
                "'svd'",
                "'cholesky'",
                "'lsqr'",
                "'sparse_cg'",
                "'sag'",
                "'saga'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:66",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "8a363dc27cdb675a4c9fc87ded4ffc43d0d06b9e73e786cb2d945c7c170917e5",
        "warmup_time": 1
    },
    "linear_model.RidgeBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.RidgeBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "representation",
            "solver"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'auto'",
                "'svd'",
                "'cholesky'",
                "'lsqr'",
                "'sparse_cg'",
                "'sag'",
                "'saga'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:66",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "6c702677952dfcf9626c6abec5f29d6cb4d5f7c833e6d66d0167375a17d8f911",
        "warmup_time": 1
    },
    "linear_model.RidgeBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.RidgeBenchmark.track_test_score",
        "param_names": [
            "representation",
            "solver"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'auto'",
                "'svd'",
                "'cholesky'",
                "'lsqr'",
                "'sparse_cg'",
                "'sag'",
                "'saga'"
            ]
        ],
        "setup_cache_key": "linear_model:66",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "b1e2308ac9f2bfe08840f9c17838fd74aef28ffaf207ed8c20d96bdb6cb2b58f"
    },
    "linear_model.RidgeBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass RidgeBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.RidgeBenchmark.track_train_score",
        "param_names": [
            "representation",
            "solver"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'auto'",
                "'svd'",
                "'cholesky'",
                "'lsqr'",
                "'sparse_cg'",
                "'sag'",
                "'saga'"
            ]
        ],
        "setup_cache_key": "linear_model:66",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "e96065aa177f3ef7660e8b1c5d33903d198a160073bb32bae3bf31859fb790e2"
    },
    "linear_model.SGDRegressorBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.SGDRegressorBenchmark.peakmem_fit",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "linear_model:141",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "68b98e0f1064163a34bc0578aa1cec92de65faceac2c895a43dc0fe28f277183"
    },
    "linear_model.SGDRegressorBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.SGDRegressorBenchmark.peakmem_predict",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "linear_model:141",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "813f1a9f4e78a8f537c3da3011432175c068f46d5f99e084797f793bb2ce0c71"
    },
    "linear_model.SGDRegressorBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.SGDRegressorBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:141",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "0746a9008d18b05e045f4be5c1fc1495f4ff577234636923a65172909250f277",
        "warmup_time": 1
    },
    "linear_model.SGDRegressorBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "linear_model.SGDRegressorBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "linear_model:141",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "92c45304d728e9fc68e0ad075acfab2527c012c257f1138fdc90becf644bddce",
        "warmup_time": 1
    },
    "linear_model.SGDRegressorBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.SGDRegressorBenchmark.track_test_score",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "linear_model:141",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "e15ee5db5de8aa8755ff4c59edd3ebea7317e15e47b4b960e8af6644fed1eed9"
    },
    "linear_model.SGDRegressorBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SGDRegressorBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "linear_model.SGDRegressorBenchmark.track_train_score",
        "param_names": [
            "representation"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ]
        ],
        "setup_cache_key": "linear_model:141",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "7732977fb570e6e6e5cc0fe2b8270dc2ce9ee4e0c50fac00d102e91072f3fcf3"
    },
    "manifold.TSNEBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass TSNEBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "manifold.TSNEBenchmark.peakmem_fit",
        "param_names": [
            "method"
        ],
        "params": [
            [
                "'exact'",
                "'barnes_hut'"
            ]
        ],
        "setup_cache_key": "manifold:15",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "68f2ed8a98bbd0198abf7ccece495dede3db25c40073344d4ff26a8921d25d09"
    },
    "manifold.TSNEBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass TSNEBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "manifold.TSNEBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "method"
        ],
        "params": [
            [
                "'exact'",
                "'barnes_hut'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "manifold:15",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "0bc09923597164ea9b4351cdbede223850c7012f81b646429e3637ea2b4fc222",
        "warmup_time": 1
    },
    "manifold.TSNEBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass TSNEBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "manifold.TSNEBenchmark.track_test_score",
        "param_names": [
            "method"
        ],
        "params": [
            [
                "'exact'",
                "'barnes_hut'"
            ]
        ],
        "setup_cache_key": "manifold:15",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "082788471a14e89a238c8e6f4bb3d5ea740773a3474b446f642868f9bad812c2"
    },
    "manifold.TSNEBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass TSNEBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "manifold.TSNEBenchmark.track_train_score",
        "param_names": [
            "method"
        ],
        "params": [
            [
                "'exact'",
                "'barnes_hut'"
            ]
        ],
        "setup_cache_key": "manifold:15",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "ea38ad09ca9102953d5956c98df7dc58fbd07b38685c6d31ce41db07c81c00b7"
    },
    "metrics.PairwiseDistancesBenchmark.peakmem_pairwise_distances": {
        "code": "class PairwiseDistancesBenchmark:\n    def peakmem_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == 'sparse' and metric == 'correlation':\n            raise NotImplementedError\n    \n        if Benchmark.data_size == 'large':\n            if metric in ('manhattan', 'correlation'):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in ('manhattan', 'correlation'):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples,\n                               representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {'metric': metric,\n                             'n_jobs': n_jobs}",
        "name": "metrics.PairwiseDistancesBenchmark.peakmem_pairwise_distances",
        "param_names": [
            "representation",
            "metric",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'cosine'",
                "'euclidean'",
                "'manhattan'",
                "'correlation'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "9e233ad5c2450fe03927e9610c457ce1c967b83279fe878e2280101e3e1ae80d"
    },
    "metrics.PairwiseDistancesBenchmark.time_pairwise_distances": {
        "code": "class PairwiseDistancesBenchmark:\n    def time_pairwise_distances(self, *args):\n        pairwise_distances(self.X, **self.pdist_params)\n\n    def setup(self, *params):\n        representation, metric, n_jobs = params\n    \n        if representation == 'sparse' and metric == 'correlation':\n            raise NotImplementedError\n    \n        if Benchmark.data_size == 'large':\n            if metric in ('manhattan', 'correlation'):\n                n_samples = 8000\n            else:\n                n_samples = 24000\n        else:\n            if metric in ('manhattan', 'correlation'):\n                n_samples = 4000\n            else:\n                n_samples = 12000\n    \n        data = _random_dataset(n_samples=n_samples,\n                               representation=representation)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.pdist_params = {'metric': metric,\n                             'n_jobs': n_jobs}",
        "min_run_count": 2,
        "name": "metrics.PairwiseDistancesBenchmark.time_pairwise_distances",
        "number": 0,
        "param_names": [
            "representation",
            "metric",
            "n_jobs"
        ],
        "params": [
            [
                "'dense'",
                "'sparse'"
            ],
            [
                "'cosine'",
                "'euclidean'",
                "'manhattan'",
                "'correlation'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "9fed496f3dad2c9dc76f028f417784b342266ee16197672e899e403a5447b57c",
        "warmup_time": 1
    },
    "model_selection.CrossValidationBenchmark.peakmem_crossval": {
        "code": "class CrossValidationBenchmark:\n    def peakmem_crossval(self, *args):\n        cross_val_score(self.clf, self.X, self.y, **self.cv_params)\n\n    def setup(self, *params):\n        n_jobs, = params\n    \n        data = _synth_classification_dataset(n_samples=50000, n_features=100)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.clf = RandomForestClassifier(n_estimators=50,\n                                          max_depth=10,\n                                          random_state=0)\n    \n        cv = 16 if Benchmark.data_size == 'large' else 4\n    \n        self.cv_params = {'n_jobs': n_jobs,\n                          'cv': cv}",
        "name": "model_selection.CrossValidationBenchmark.peakmem_crossval",
        "param_names": [
            "n_jobs"
        ],
        "params": [
            [
                "1",
                "4"
            ]
        ],
        "timeout": 20000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "6801caf1f8cd21ed17a0095ee8a6a4f32e18fc9c4fda5efc812cd8c88053146b"
    },
    "model_selection.CrossValidationBenchmark.time_crossval": {
        "code": "class CrossValidationBenchmark:\n    def time_crossval(self, *args):\n        cross_val_score(self.clf, self.X, self.y, **self.cv_params)\n\n    def setup(self, *params):\n        n_jobs, = params\n    \n        data = _synth_classification_dataset(n_samples=50000, n_features=100)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.clf = RandomForestClassifier(n_estimators=50,\n                                          max_depth=10,\n                                          random_state=0)\n    \n        cv = 16 if Benchmark.data_size == 'large' else 4\n    \n        self.cv_params = {'n_jobs': n_jobs,\n                          'cv': cv}",
        "min_run_count": 2,
        "name": "model_selection.CrossValidationBenchmark.time_crossval",
        "number": 0,
        "param_names": [
            "n_jobs"
        ],
        "params": [
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "timeout": 20000,
        "type": "time",
        "unit": "seconds",
        "version": "be3520243b65d79e9c474fd2297b14127cf9de93ebc3c36d5cbf473e515a7f51",
        "warmup_time": 1
    },
    "model_selection.CrossValidationBenchmark.track_crossval": {
        "code": "class CrossValidationBenchmark:\n    def track_crossval(self, *args):\n        return float(cross_val_score(self.clf, self.X,\n                                     self.y, **self.cv_params).mean())\n\n    def setup(self, *params):\n        n_jobs, = params\n    \n        data = _synth_classification_dataset(n_samples=50000, n_features=100)\n        self.X, self.X_val, self.y, self.y_val = data\n    \n        self.clf = RandomForestClassifier(n_estimators=50,\n                                          max_depth=10,\n                                          random_state=0)\n    \n        cv = 16 if Benchmark.data_size == 'large' else 4\n    \n        self.cv_params = {'n_jobs': n_jobs,\n                          'cv': cv}",
        "name": "model_selection.CrossValidationBenchmark.track_crossval",
        "param_names": [
            "n_jobs"
        ],
        "params": [
            [
                "1",
                "4"
            ]
        ],
        "timeout": 20000,
        "type": "track",
        "unit": "unit",
        "version": "74d326542acd7965a4ac38cd7dcbf848a4ee175d5aee479001a1e7846287e113"
    },
    "model_selection.GridSearchBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "model_selection.GridSearchBenchmark.peakmem_fit",
        "param_names": [
            "n_jobs"
        ],
        "params": [
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "model_selection:55",
        "timeout": 20000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d2dd984f5f9ffe61ece803d4ee2f05375513c546c7de765e24ec1edff3359d75"
    },
    "model_selection.GridSearchBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "model_selection.GridSearchBenchmark.peakmem_predict",
        "param_names": [
            "n_jobs"
        ],
        "params": [
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "model_selection:55",
        "timeout": 20000,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "0a181b0ce7fbc54bf6b7032db6b39562542ddef6a35255668b9a088799e225c3"
    },
    "model_selection.GridSearchBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "model_selection.GridSearchBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "n_jobs"
        ],
        "params": [
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "model_selection:55",
        "timeout": 20000,
        "type": "time",
        "unit": "seconds",
        "version": "6e8041bfb2a873d368978c028fbe561790c06c5a0d0ec5e6110543525f5cb9a2",
        "warmup_time": 1
    },
    "model_selection.GridSearchBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "model_selection.GridSearchBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "n_jobs"
        ],
        "params": [
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "model_selection:55",
        "timeout": 20000,
        "type": "time",
        "unit": "seconds",
        "version": "dcdea000aea14892ba793d2354b387d4a23d187fcdc6d32e167d544ba4e2b0cc",
        "warmup_time": 1
    },
    "model_selection.GridSearchBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "model_selection.GridSearchBenchmark.track_test_score",
        "param_names": [
            "n_jobs"
        ],
        "params": [
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "model_selection:55",
        "timeout": 20000,
        "type": "track",
        "unit": "unit",
        "version": "6469d2e25b08248849a365b43fd7c9f442201399d7a690e1962a95df3b39722f"
    },
    "model_selection.GridSearchBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass GridSearchBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "model_selection.GridSearchBenchmark.track_train_score",
        "param_names": [
            "n_jobs"
        ],
        "params": [
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "model_selection:55",
        "timeout": 20000,
        "type": "track",
        "unit": "unit",
        "version": "e3c89e4a58c0cae0a5999ef85ae2809732335d78588dd2e02b3005e5535606fb"
    },
    "neighbors.KNeighborsClassifierBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "neighbors.KNeighborsClassifierBenchmark.peakmem_fit",
        "param_names": [
            "algorithm",
            "dimension",
            "n_jobs"
        ],
        "params": [
            [
                "'brute'",
                "'kd_tree'",
                "'ball_tree'"
            ],
            [
                "'low'",
                "'high'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "neighbors:18",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "43360d17ecddfc1659b798464839c3bac0e83d375aa1053cf4cbc45f2107f868"
    },
    "neighbors.KNeighborsClassifierBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "neighbors.KNeighborsClassifierBenchmark.peakmem_predict",
        "param_names": [
            "algorithm",
            "dimension",
            "n_jobs"
        ],
        "params": [
            [
                "'brute'",
                "'kd_tree'",
                "'ball_tree'"
            ],
            [
                "'low'",
                "'high'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "neighbors:18",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "a9b867976f10c5b9de77e884b025c937079c4736d48a0a916aafb215ccd5c6b5"
    },
    "neighbors.KNeighborsClassifierBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "neighbors.KNeighborsClassifierBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "algorithm",
            "dimension",
            "n_jobs"
        ],
        "params": [
            [
                "'brute'",
                "'kd_tree'",
                "'ball_tree'"
            ],
            [
                "'low'",
                "'high'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "neighbors:18",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "b69699ce3d079dbb3cdb110c8538804c4b1121ef6f38f473979b0063bbd0c42b",
        "warmup_time": 1
    },
    "neighbors.KNeighborsClassifierBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "neighbors.KNeighborsClassifierBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "algorithm",
            "dimension",
            "n_jobs"
        ],
        "params": [
            [
                "'brute'",
                "'kd_tree'",
                "'ball_tree'"
            ],
            [
                "'low'",
                "'high'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "neighbors:18",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "e855c72d2ecca559cf5a5b0281ee724eaffc87fd376347c28ae6d9007b7889a4",
        "warmup_time": 1
    },
    "neighbors.KNeighborsClassifierBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "neighbors.KNeighborsClassifierBenchmark.track_test_score",
        "param_names": [
            "algorithm",
            "dimension",
            "n_jobs"
        ],
        "params": [
            [
                "'brute'",
                "'kd_tree'",
                "'ball_tree'"
            ],
            [
                "'low'",
                "'high'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "neighbors:18",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "7579889d2fe88fa901e741f40636a3145f8df3dc88ddbc26962d150f48a68a80"
    },
    "neighbors.KNeighborsClassifierBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass KNeighborsClassifierBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "neighbors.KNeighborsClassifierBenchmark.track_train_score",
        "param_names": [
            "algorithm",
            "dimension",
            "n_jobs"
        ],
        "params": [
            [
                "'brute'",
                "'kd_tree'",
                "'ball_tree'"
            ],
            [
                "'low'",
                "'high'"
            ],
            [
                "1",
                "4"
            ]
        ],
        "setup_cache_key": "neighbors:18",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "08d43de9f99e9a3f1640453df03be3a7dd66740bc8ad71de8f6931c856f9815f"
    },
    "svm.SVCBenchmark.peakmem_fit": {
        "code": "class Estimator:\n    def peakmem_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "svm.SVCBenchmark.peakmem_fit",
        "param_names": [
            "kernel"
        ],
        "params": [
            [
                "'linear'",
                "'poly'",
                "'rbf'",
                "'sigmoid'"
            ]
        ],
        "setup_cache_key": "svm:14",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "d61868e19dbb13fb2fe3bf053eb8b41c8a099a959f559ff9ca2d4e96639488ec"
    },
    "svm.SVCBenchmark.peakmem_predict": {
        "code": "class Predictor:\n    def peakmem_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "svm.SVCBenchmark.peakmem_predict",
        "param_names": [
            "kernel"
        ],
        "params": [
            [
                "'linear'",
                "'poly'",
                "'rbf'",
                "'sigmoid'"
            ]
        ],
        "setup_cache_key": "svm:14",
        "timeout": 500,
        "type": "peakmemory",
        "unit": "bytes",
        "version": "7bf277d2275581194a28809fe59ba881f789d3251b6934680957c18ec833bed8"
    },
    "svm.SVCBenchmark.time_fit": {
        "code": "class Estimator:\n    def time_fit(self, *args):\n        self.estimator.fit(self.X, self.y)\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "svm.SVCBenchmark.time_fit",
        "number": 0,
        "param_names": [
            "kernel"
        ],
        "params": [
            [
                "'linear'",
                "'poly'",
                "'rbf'",
                "'sigmoid'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "svm:14",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "87b16eef694e92177abd53b94cf22434f4d1d2b24472384edbba12f3ffd79e5d",
        "warmup_time": 1
    },
    "svm.SVCBenchmark.time_predict": {
        "code": "class Predictor:\n    def time_predict(self, *args):\n        self.estimator.predict(self.X)\n\nclass Estimator:\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "min_run_count": 2,
        "name": "svm.SVCBenchmark.time_predict",
        "number": 0,
        "param_names": [
            "kernel"
        ],
        "params": [
            [
                "'linear'",
                "'poly'",
                "'rbf'",
                "'sigmoid'"
            ]
        ],
        "rounds": 1,
        "sample_time": 0.01,
        "setup_cache_key": "svm:14",
        "timeout": 500,
        "type": "time",
        "unit": "seconds",
        "version": "008eecb5025ad842ac7693131008e28a6fedfb423560d9692b596b7cb69f905a",
        "warmup_time": 1
    },
    "svm.SVCBenchmark.track_test_score": {
        "code": "class Estimator:\n    def track_test_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_val_pred = self.estimator.predict(self.X_val)\n        else:\n            y_val_pred = None\n        return float(self.test_scorer(self.y_val, y_val_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "svm.SVCBenchmark.track_test_score",
        "param_names": [
            "kernel"
        ],
        "params": [
            [
                "'linear'",
                "'poly'",
                "'rbf'",
                "'sigmoid'"
            ]
        ],
        "setup_cache_key": "svm:14",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "9bfdd3bfb929bf4d6c394a4e6b3e766d2887a161011ebeff3a32e20d4f46dc98"
    },
    "svm.SVCBenchmark.track_train_score": {
        "code": "class Estimator:\n    def track_train_score(self, *args):\n        if hasattr(self.estimator, 'predict'):\n            y_pred = self.estimator.predict(self.X)\n        else:\n            y_pred = None\n        return float(self.train_scorer(self.y, y_pred))\n\n    def setup(self, *params):\n        \"\"\"Generate dataset and load the fitted estimator\"\"\"\n        # This is run once per combination of parameters and per repeat so we\n        # need to avoid doing expensive operations there.\n    \n        if self.skip(params):\n            raise NotImplementedError\n    \n        self.X, self.X_val, self.y, self.y_val = self.make_data(params)\n    \n        est_path = get_estimator_path(self, Benchmark.save_dir,\n                                      params, Benchmark.save_estimators)\n        with est_path.open(mode='rb') as f:\n            self.estimator = pickle.load(f)\n    \n        self.make_scorers()\n\nclass SVCBenchmark:\n    def setup_cache(self):\n        super().setup_cache()",
        "name": "svm.SVCBenchmark.track_train_score",
        "param_names": [
            "kernel"
        ],
        "params": [
            [
                "'linear'",
                "'poly'",
                "'rbf'",
                "'sigmoid'"
            ]
        ],
        "setup_cache_key": "svm:14",
        "timeout": 500,
        "type": "track",
        "unit": "unit",
        "version": "e43292f2ef23d0a9c445b7e69128d96e90ea2522bdd8858eae0051bc20d54385"
    },
    "version": 2
}